{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c6694a",
   "metadata": {},
   "source": [
    "## =============================================================================\n",
    "## NOTEBOOK 01 - PRÃ‰PARATION DES DONNÃ‰ES\n",
    "## =============================================================================\n",
    "\n",
    "## ðŸ“Š PrÃ©paration des DonnÃ©es pour Drone Agri AI\n",
    "\n",
    "## Ce notebook couvre:\n",
    "## 1. TÃ©lÃ©chargement des datasets\n",
    "## 2. Exploration et visualisation\n",
    "## 3. Nettoyage et prÃ©traitement\n",
    "## 4. Augmentation des donnÃ©es\n",
    "## 5. Division train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "!pip install -q kaggle albumentations matplotlib seaborn\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b9b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Chemins\n",
    "PROJECT_DIR = Path('/content/drone-agri-ai')\n",
    "DATA_DIR = Path('/content/data')\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "for d in [PROJECT_DIR, DATA_DIR, RAW_DIR, PROCESSED_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project: {PROJECT_DIR}\")\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "\n",
    "# Configuration Kaggle\n",
    "# MÃ©thode 1: Upload manuel\n",
    "from google.colab import files\n",
    "print(\"Uploadez votre fichier kaggle.json:\")\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# MÃ©thode 2: Depuis Google Drive\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/ 2>/dev/null || echo \"Fichier non trouvÃ© dans Drive\"\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# VÃ©rifier\n",
    "!kaggle datasets list -s plant --sort-by votes | head -10\n",
    "\n",
    "# TÃ©lÃ©charger les datasets\n",
    "print(\"ðŸ“¥ TÃ©lÃ©chargement des datasets...\")\n",
    "\n",
    "# PlantVillage - Principal dataset\n",
    "!kaggle datasets download -d abdallahalidev/plantvillage-dataset -p {RAW_DIR} --unzip\n",
    "\n",
    "# New Plant Diseases Dataset\n",
    "!kaggle datasets download -d vipoooool/new-plant-diseases-dataset -p {RAW_DIR} --unzip\n",
    "\n",
    "# Plant Seedlings (optionnel)\n",
    "# !kaggle datasets download -d vbookshelf/v2-plant-seedlings-dataset -p {RAW_DIR} --unzip\n",
    "\n",
    "print(\"âœ… TÃ©lÃ©chargement terminÃ©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorer la structure\n",
    "def explore_directory(path, max_depth=3, current_depth=0):\n",
    "    \"\"\"Explore rÃ©cursivement un rÃ©pertoire\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    path = Path(path)\n",
    "    indent = \"  \" * current_depth\n",
    "    \n",
    "    for item in sorted(path.iterdir())[:20]:  # Limiter Ã  20 items\n",
    "        if item.is_dir():\n",
    "            count = len(list(item.glob('*')))\n",
    "            print(f\"{indent}ðŸ“ {item.name}/ ({count} items)\")\n",
    "            explore_directory(item, max_depth, current_depth + 1)\n",
    "        else:\n",
    "            size = item.stat().st_size / 1024  # KB\n",
    "            print(f\"{indent}ðŸ“„ {item.name} ({size:.1f} KB)\")\n",
    "\n",
    "print(\"Structure du dossier data:\")\n",
    "explore_directory(DATA_DIR, max_depth=3)\n",
    "\n",
    "# Analyser PlantVillage\n",
    "# Trouver le dossier des images\n",
    "plantvillage_paths = list(RAW_DIR.glob('**/color')) + list(RAW_DIR.glob('**/Color'))\n",
    "if not plantvillage_paths:\n",
    "    plantvillage_paths = list(RAW_DIR.glob('**/*'))\n",
    "    \n",
    "print(f\"Chemins trouvÃ©s: {plantvillage_paths[:5]}\")\n",
    "\n",
    "# Utiliser le premier chemin valide\n",
    "data_path = None\n",
    "for p in plantvillage_paths:\n",
    "    if p.is_dir() and len(list(p.iterdir())) > 10:\n",
    "        data_path = p\n",
    "        break\n",
    "\n",
    "if data_path is None:\n",
    "    # Chercher directement les dossiers de classes\n",
    "    for p in RAW_DIR.rglob('*'):\n",
    "        if p.is_dir():\n",
    "            subdirs = list(p.iterdir())\n",
    "            if len(subdirs) > 20 and all(d.is_dir() for d in subdirs[:5]):\n",
    "                data_path = p\n",
    "                break\n",
    "\n",
    "print(f\"ðŸ“‚ Dossier de donnÃ©es: {data_path}\")\n",
    "\n",
    "# Statistiques des classes\n",
    "classes = sorted([d.name for d in data_path.iterdir() if d.is_dir()])\n",
    "print(f\"\\nðŸ“Š Nombre de classes: {len(classes)}\")\n",
    "\n",
    "# Compter les images par classe\n",
    "class_counts = {}\n",
    "for class_name in tqdm(classes, desc=\"Comptage\"):\n",
    "    class_dir = data_path / class_name\n",
    "    count = len(list(class_dir.glob('*.jpg'))) + len(list(class_dir.glob('*.JPG'))) + len(list(class_dir.glob('*.png')))\n",
    "    class_counts[class_name] = count\n",
    "\n",
    "# DataFrame pour analyse\n",
    "df_classes = pd.DataFrame([\n",
    "    {\"class\": k, \"count\": v} for k, v in class_counts.items()\n",
    "])\n",
    "df_classes = df_classes.sort_values('count', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Total d'images: {df_classes['count'].sum()}\")\n",
    "print(f\"ðŸ“ˆ Moyenne par classe: {df_classes['count'].mean():.0f}\")\n",
    "print(f\"ðŸ“ˆ Min: {df_classes['count'].min()}\")\n",
    "print(f\"ðŸ“ˆ Max: {df_classes['count'].max()}\")\n",
    "\n",
    "# Visualiser la distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Distribution des classes\n",
    "ax1 = axes[0]\n",
    "df_classes.plot(x='class', y='count', kind='bar', ax=ax1, legend=False)\n",
    "ax1.set_title('Distribution des images par classe', fontsize=14)\n",
    "ax1.set_xlabel('Classe')\n",
    "ax1.set_ylabel('Nombre d\\'images')\n",
    "ax1.tick_params(axis='x', rotation=90, labelsize=6)\n",
    "\n",
    "# Histogramme\n",
    "ax2 = axes[1]\n",
    "ax2.hist(df_classes['count'], bins=30, edgecolor='black')\n",
    "ax2.set_title('Histogramme des tailles de classes', fontsize=14)\n",
    "ax2.set_xlabel('Nombre d\\'images')\n",
    "ax2.set_ylabel('FrÃ©quence')\n",
    "ax2.axvline(df_classes['count'].mean(), color='red', linestyle='--', label='Moyenne')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_DIR / 'class_distribution.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Parser les noms de classes\n",
    "def parse_class_name(name):\n",
    "    \"\"\"Extrait la plante et la condition du nom de classe\"\"\"\n",
    "    parts = name.split('___')\n",
    "    if len(parts) == 2:\n",
    "        return {\n",
    "            'plant': parts[0].replace('_', ' '),\n",
    "            'condition': parts[1].replace('_', ' '),\n",
    "            'is_healthy': 'healthy' in parts[1].lower()\n",
    "        }\n",
    "    return {\n",
    "        'plant': name,\n",
    "        'condition': 'unknown',\n",
    "        'is_healthy': None\n",
    "    }\n",
    "\n",
    "# Analyser toutes les classes\n",
    "class_info = {}\n",
    "plants = set()\n",
    "conditions = set()\n",
    "\n",
    "for class_name in classes:\n",
    "    info = parse_class_name(class_name)\n",
    "    class_info[class_name] = info\n",
    "    plants.add(info['plant'])\n",
    "    conditions.add(info['condition'])\n",
    "\n",
    "print(f\"\\nðŸŒ± Plantes uniques: {len(plants)}\")\n",
    "for p in sorted(plants):\n",
    "    print(f\"  - {p}\")\n",
    "\n",
    "print(f\"\\nðŸ¥ Conditions uniques: {len(conditions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb94322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser des exemples\n",
    "def show_samples(data_path, classes, samples_per_class=3):\n",
    "    \"\"\"Affiche des exemples de chaque classe\"\"\"\n",
    "    n_classes = min(12, len(classes))\n",
    "    fig, axes = plt.subplots(n_classes, samples_per_class, figsize=(samples_per_class * 3, n_classes * 2.5))\n",
    "    \n",
    "    for i, class_name in enumerate(classes[:n_classes]):\n",
    "        class_dir = data_path / class_name\n",
    "        images = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.JPG'))\n",
    "        \n",
    "        for j in range(samples_per_class):\n",
    "            ax = axes[i, j] if n_classes > 1 else axes[j]\n",
    "            \n",
    "            if j < len(images):\n",
    "                img = cv2.imread(str(images[j]))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                ax.imshow(img)\n",
    "            \n",
    "            ax.axis('off')\n",
    "            if j == 0:\n",
    "                ax.set_title(class_name[:30], fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PROJECT_DIR / 'sample_images.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "show_samples(data_path, classes[:12])\n",
    "\n",
    "# VÃ©rifier la qualitÃ© des images\n",
    "print(\"ðŸ” VÃ©rification de la qualitÃ© des images...\")\n",
    "\n",
    "issues = []\n",
    "image_sizes = []\n",
    "image_formats = Counter()\n",
    "\n",
    "for class_name in tqdm(classes[:10], desc=\"VÃ©rification\"):  # Limiter pour rapiditÃ©\n",
    "    class_dir = data_path / class_name\n",
    "    \n",
    "    for img_path in list(class_dir.glob('*'))[:50]:  # 50 images par classe\n",
    "        try:\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                issues.append(f\"Cannot read: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            h, w = img.shape[:2]\n",
    "            image_sizes.append((w, h))\n",
    "            image_formats[img_path.suffix.lower()] += 1\n",
    "            \n",
    "            # VÃ©rifier les dimensions minimales\n",
    "            if w < 50 or h < 50:\n",
    "                issues.append(f\"Too small ({w}x{h}): {img_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            issues.append(f\"Error {e}: {img_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Formats trouvÃ©s: {dict(image_formats)}\")\n",
    "print(f\"âš ï¸ ProblÃ¨mes dÃ©tectÃ©s: {len(issues)}\")\n",
    "\n",
    "if issues:\n",
    "    print(\"\\nExemples de problÃ¨mes:\")\n",
    "    for issue in issues[:5]:\n",
    "        print(f\"  - {issue}\")\n",
    "\n",
    "# Statistiques de taille\n",
    "sizes = np.array(image_sizes)\n",
    "print(f\"\\nðŸ“ Tailles d'images:\")\n",
    "print(f\"  Min: {sizes.min(axis=0)}\")\n",
    "print(f\"  Max: {sizes.max(axis=0)}\")\n",
    "print(f\"  Moyenne: {sizes.mean(axis=0).astype(int)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrÃ©parer le dataset final\n",
    "print(\"ðŸ“¦ PrÃ©paration du dataset final...\")\n",
    "\n",
    "# Structure cible\n",
    "TRAIN_DIR = PROCESSED_DIR / 'train'\n",
    "VAL_DIR = PROCESSED_DIR / 'val'\n",
    "TEST_DIR = PROCESSED_DIR / 'test'\n",
    "\n",
    "for d in [TRAIN_DIR, VAL_DIR, TEST_DIR]:\n",
    "    if d.exists():\n",
    "        shutil.rmtree(d)\n",
    "    d.mkdir(parents=True)\n",
    "\n",
    "# Ratios\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Copier les images\n",
    "np.random.seed(42)\n",
    "\n",
    "for class_name in tqdm(classes, desc=\"Copie des images\"):\n",
    "    src_dir = data_path / class_name\n",
    "    images = list(src_dir.glob('*.jpg')) + list(src_dir.glob('*.JPG')) + list(src_dir.glob('*.png'))\n",
    "    \n",
    "    np.random.shuffle(images)\n",
    "    \n",
    "    n = len(images)\n",
    "    n_train = int(n * TRAIN_RATIO)\n",
    "    n_val = int(n * VAL_RATIO)\n",
    "    \n",
    "    splits = {\n",
    "        'train': images[:n_train],\n",
    "        'val': images[n_train:n_train + n_val],\n",
    "        'test': images[n_train + n_val:]\n",
    "    }\n",
    "    \n",
    "    for split_name, split_images in splits.items():\n",
    "        dest_dir = PROCESSED_DIR / split_name / class_name\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for img_path in split_images:\n",
    "            shutil.copy2(img_path, dest_dir / img_path.name)\n",
    "\n",
    "# VÃ©rifier le split\n",
    "def count_split(split_dir):\n",
    "    \"\"\"Compte les images dans un split\"\"\"\n",
    "    total = 0\n",
    "    for class_dir in split_dir.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            total += len(list(class_dir.glob('*')))\n",
    "    return total\n",
    "\n",
    "print(\"\\nðŸ“Š RÃ©sumÃ© du split:\")\n",
    "print(f\"  Train: {count_split(TRAIN_DIR)} images\")\n",
    "print(f\"  Val: {count_split(VAL_DIR)} images\")\n",
    "print(f\"  Test: {count_split(TEST_DIR)} images\")\n",
    "\n",
    "# Sauvegarder les mÃ©tadonnÃ©es\n",
    "metadata = {\n",
    "    \"created_at\": pd.Timestamp.now().isoformat(),\n",
    "    \"source\": \"PlantVillage\",\n",
    "    \"num_classes\": len(classes),\n",
    "    \"classes\": classes,\n",
    "    \"class_info\": class_info,\n",
    "    \"splits\": {\n",
    "        \"train\": count_split(TRAIN_DIR),\n",
    "        \"val\": count_split(VAL_DIR),\n",
    "        \"test\": count_split(TEST_DIR)\n",
    "    },\n",
    "    \"plants\": list(plants),\n",
    "    \"input_size\": [224, 224]\n",
    "}\n",
    "\n",
    "with open(PROCESSED_DIR / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"âœ… MÃ©tadonnÃ©es sauvegardÃ©es!\")\n",
    "\n",
    "# Copier sur Google Drive\n",
    "!cp -r {PROCESSED_DIR}/* /content/drive/MyDrive/drone-agri-ai/data/\n",
    "print(\"âœ… DonnÃ©es copiÃ©es sur Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ae1df",
   "metadata": {},
   "source": [
    "# ## âœ… DonnÃ©es prÃªtes!\n",
    "# \n",
    "# Le dataset est maintenant prÃªt pour l'entraÃ®nement:\n",
    "# - Images organisÃ©es en train/val/test\n",
    "# - MÃ©tadonnÃ©es sauvegardÃ©es\n",
    "# - Copie sur Google Drive"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
