{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c6694a",
   "metadata": {},
   "source": [
    "### =============================================\n",
    "### NOTEBOOK 01 - PRÃ‰PARATION DES DONNÃ‰ES\n",
    "### =============================================\n",
    "\n",
    "### ðŸ“Š PrÃ©paration des DonnÃ©es pour Drone Agri AI\n",
    "\n",
    "### Ce notebook couvre:\n",
    "### 1. TÃ©lÃ©chargement des datasets\n",
    "### 2. Exploration et visualisation\n",
    "### 3. Nettoyage et prÃ©traitement\n",
    "### 4. Augmentation des donnÃ©es\n",
    "### 5. Division train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION DE L'ENVIRONNEMENT (Compatible Local + Colab)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# === DÃ‰TECTION DE L'ENVIRONNEMENT ===\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"ðŸ–¥ï¸ Environnement: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "\n",
    "# === CONFIGURATION DES CHEMINS ===\n",
    "if IN_COLAB:\n",
    "    # --- COLAB ---\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Chemin personnalisable par l'utilisateur\n",
    "    DRIVE_BASE = '/content/drive/MyDrive/Stage-X4/Drone-Agricole'\n",
    "    \n",
    "    PROJECT_DIR = Path('/content/drone-agri-ai')\n",
    "    DATA_DIR = Path('/content/data')\n",
    "    SAVE_DIR = Path(DRIVE_BASE) / 'drone-agri-ai'\n",
    "else:\n",
    "    # --- LOCAL ---\n",
    "    # DÃ©terminer le dossier du projet\n",
    "    if Path.cwd().name == 'notebooks':\n",
    "        PROJECT_DIR = Path.cwd().parent\n",
    "    else:\n",
    "        PROJECT_DIR = Path.cwd()\n",
    "    \n",
    "    DATA_DIR = PROJECT_DIR / 'data'\n",
    "    SAVE_DIR = PROJECT_DIR\n",
    "\n",
    "# Dossiers communs\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "MODELS_DIR = SAVE_DIR / 'models'\n",
    "\n",
    "# CrÃ©er les dossiers\n",
    "for d in [DATA_DIR, RAW_DIR, PROCESSED_DIR, MODELS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“‚ Projet: {PROJECT_DIR}\")\n",
    "print(f\"ðŸ“‚ DonnÃ©es: {DATA_DIR}\")\n",
    "print(f\"ðŸ“‚ Sauvegarde: {SAVE_DIR}\")\n",
    "print(f\"ðŸ“‚ ModÃ¨les: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba5a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TÃ‰LÃ‰CHARGEMENT DES DATASETS (Kaggle ou lien Drive)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Option 1: TÃ©lÃ©chargement Kaggle (nÃ©cessite kaggle.json)\n",
    "# Option 2: Lien Google Drive (pour utilisateur local sans Kaggle)\n",
    "\n",
    "USE_KAGGLE = True  # â† Mettre False si tu veux utiliser un lien Drive\n",
    "\n",
    "# Lien Google Drive vers un zip des datasets (optionnel)\n",
    "DRIVE_DATASET_URL = \"\"  # â† Mettre l'ID du fichier si USE_KAGGLE = False\n",
    "# Exemple: \"1ABC123xyz\" pour https://drive.google.com/file/d/1ABC123xyz/view\n",
    "\n",
    "# === LISTE DES DATASETS Ã€ TÃ‰LÃ‰CHARGER ===\n",
    "DATASETS = [\n",
    "    (\"abdallahalidev/plantvillage-dataset\", \"plantvillage\"),\n",
    "    (\"vipoooool/new-plant-diseases-dataset\", \"new-plant-diseases\"),\n",
    "    (\"vbookshelf/v2-plant-seedlings-dataset\", \"plant-seedlings\"),\n",
    "]\n",
    "\n",
    "def download_with_kaggle():\n",
    "    \"\"\"TÃ©lÃ©charge les datasets via Kaggle API\"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    # Configurer Kaggle\n",
    "    kaggle_dir = Path.home() / '.kaggle'\n",
    "    kaggle_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Chercher kaggle.json\n",
    "    possible_paths = [\n",
    "        kaggle_dir / 'kaggle.json',\n",
    "        Path('/content/drive/MyDrive/kaggle.json') if IN_COLAB else None,\n",
    "        Path('/content/drive/MyDrive/Stage-X4/Drone-Agricole/kaggle.json') if IN_COLAB else None,\n",
    "        PROJECT_DIR / 'kaggle.json',\n",
    "        Path.home() / 'kaggle.json',\n",
    "    ]\n",
    "    \n",
    "    kaggle_json = None\n",
    "    for p in possible_paths:\n",
    "        if p and p.exists():\n",
    "            kaggle_json = p\n",
    "            break\n",
    "    \n",
    "    if kaggle_json is None:\n",
    "        print(\"âŒ kaggle.json non trouvÃ©!\")\n",
    "        print(\"ðŸ“‹ Instructions:\")\n",
    "        print(\"   1. Va sur https://www.kaggle.com â†’ Settings â†’ API â†’ Create New Token\")\n",
    "        print(\"   2. Place kaggle.json dans:\", kaggle_dir)\n",
    "        return False\n",
    "    \n",
    "    # Copier vers ~/.kaggle si nÃ©cessaire\n",
    "    if kaggle_json != kaggle_dir / 'kaggle.json':\n",
    "        import shutil\n",
    "        shutil.copy(kaggle_json, kaggle_dir / 'kaggle.json')\n",
    "    os.chmod(kaggle_dir / 'kaggle.json', 0o600)\n",
    "    \n",
    "    print(\"âœ… Kaggle configurÃ©!\")\n",
    "    \n",
    "    # TÃ©lÃ©charger chaque dataset\n",
    "    for dataset_name, folder_name in DATASETS:\n",
    "        dest = RAW_DIR / folder_name\n",
    "        dest.mkdir(exist_ok=True)\n",
    "        \n",
    "        # VÃ©rifier si dÃ©jÃ  tÃ©lÃ©chargÃ©\n",
    "        if any(dest.iterdir()):\n",
    "            print(f\"â­ï¸ {folder_name} dÃ©jÃ  prÃ©sent, skip...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"ðŸ“¥ TÃ©lÃ©chargement: {dataset_name}...\")\n",
    "        try:\n",
    "            subprocess.run([\n",
    "                'kaggle', 'datasets', 'download', '-d', dataset_name,\n",
    "                '-p', str(dest), '--unzip', '-q'\n",
    "            ], check=True)\n",
    "            print(f\"   âœ… {folder_name} tÃ©lÃ©chargÃ©!\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Erreur: {e}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def download_from_drive(file_id):\n",
    "    \"\"\"TÃ©lÃ©charge depuis Google Drive (pour utilisateurs sans Kaggle)\"\"\"\n",
    "    import urllib.request\n",
    "    import zipfile\n",
    "    \n",
    "    url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "    zip_path = DATA_DIR / 'datasets.zip'\n",
    "    \n",
    "    print(f\"ðŸ“¥ TÃ©lÃ©chargement depuis Google Drive...\")\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    \n",
    "    print(\"ðŸ“¦ Extraction...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extractall(RAW_DIR)\n",
    "    \n",
    "    zip_path.unlink()\n",
    "    print(\"âœ… Datasets extraits!\")\n",
    "    return True\n",
    "\n",
    "# === EXÃ‰CUTION ===\n",
    "if USE_KAGGLE:\n",
    "    success = download_with_kaggle()\n",
    "else:\n",
    "    if DRIVE_DATASET_URL:\n",
    "        success = download_from_drive(DRIVE_DATASET_URL)\n",
    "    else:\n",
    "        print(\"âš ï¸ Aucune source de donnÃ©es configurÃ©e!\")\n",
    "        print(\"   Mets USE_KAGGLE = True ou fournis DRIVE_DATASET_URL\")\n",
    "        success = False\n",
    "\n",
    "if success:\n",
    "    print(\"\\nðŸ“ Contenu de RAW_DIR:\")\n",
    "    for item in RAW_DIR.iterdir():\n",
    "        if item.is_dir():\n",
    "            count = sum(1 for _ in item.rglob('*') if _.is_file())\n",
    "            print(f\"   ðŸ“‚ {item.name}: {count} fichiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TROUVER LE DOSSIER PRINCIPAL DES IMAGES ===\n",
    "\n",
    "def find_data_folder(base_dir):\n",
    "    \"\"\"Trouve automatiquement le dossier contenant les classes d'images\"\"\"\n",
    "    base_dir = Path(base_dir)\n",
    "    \n",
    "    # Chemins possibles selon la structure des datasets\n",
    "    candidates = [\n",
    "        base_dir / 'plantvillage' / 'plantvillage dataset' / 'color',\n",
    "        base_dir / 'plantvillage' / 'PlantVillage',\n",
    "        base_dir / 'plantvillage' / 'color',\n",
    "        base_dir / 'new-plant-diseases' / 'New Plant Diseases Dataset(Augmented)' / 'train',\n",
    "        base_dir / 'new-plant-diseases' / 'train',\n",
    "        base_dir / 'color',\n",
    "        base_dir / 'train',\n",
    "    ]\n",
    "    \n",
    "    for path in candidates:\n",
    "        if path.exists():\n",
    "            subdirs = [d for d in path.iterdir() if d.is_dir()]\n",
    "            if len(subdirs) > 5:  # Au moins quelques classes\n",
    "                return path\n",
    "    \n",
    "    # Recherche automatique rÃ©cursive\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        if len(dirs) > 15:  # Beaucoup de sous-dossiers = classes\n",
    "            sample = Path(root) / dirs[0]\n",
    "            images = list(sample.glob('*.jpg')) + list(sample.glob('*.JPG')) + list(sample.glob('*.png'))\n",
    "            if len(images) > 5:\n",
    "                return Path(root)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Trouver le dossier\n",
    "data_path = find_data_folder(RAW_DIR)\n",
    "\n",
    "if data_path is None:\n",
    "    print(\"âŒ Dossier de donnÃ©es non trouvÃ©!\")\n",
    "    print(\"ðŸ“ Contenu de RAW_DIR:\")\n",
    "    if RAW_DIR.exists():\n",
    "        for item in RAW_DIR.iterdir():\n",
    "            print(f\"   - {item.name}\")\n",
    "    raise FileNotFoundError(\"Veuillez vÃ©rifier que les datasets sont tÃ©lÃ©chargÃ©s\")\n",
    "\n",
    "print(f\"ðŸ“‚ Dossier de donnÃ©es: {data_path}\")\n",
    "\n",
    "# Statistiques des classes\n",
    "classes = sorted([d.name for d in data_path.iterdir() if d.is_dir()])\n",
    "print(f\"\\nðŸ“Š Nombre de classes: {len(classes)}\")\n",
    "\n",
    "# Compter les images par classe\n",
    "class_counts = {}\n",
    "for class_name in tqdm(classes, desc=\"Comptage\"):\n",
    "    class_dir = data_path / class_name\n",
    "    count = len(list(class_dir.glob('*.jpg'))) + len(list(class_dir.glob('*.JPG'))) + len(list(class_dir.glob('*.png')))\n",
    "    class_counts[class_name] = count\n",
    "\n",
    "# DataFrame pour analyse\n",
    "df_classes = pd.DataFrame([\n",
    "    {\"class\": k, \"count\": v} for k, v in class_counts.items()\n",
    "])\n",
    "df_classes = df_classes.sort_values('count', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Total d'images: {df_classes['count'].sum()}\")\n",
    "print(f\"ðŸ“ˆ Moyenne par classe: {df_classes['count'].mean():.0f}\")\n",
    "print(f\"ðŸ“ˆ Min: {df_classes['count'].min()}\")\n",
    "print(f\"ðŸ“ˆ Max: {df_classes['count'].max()}\")\n",
    "\n",
    "# Visualiser la distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Distribution des classes\n",
    "ax1 = axes[0]\n",
    "df_classes.plot(x='class', y='count', kind='bar', ax=ax1, legend=False)\n",
    "ax1.set_title('Distribution des images par classe', fontsize=14)\n",
    "ax1.set_xlabel('Classe')\n",
    "ax1.set_ylabel('Nombre d\\'images')\n",
    "ax1.tick_params(axis='x', rotation=90, labelsize=6)\n",
    "\n",
    "# Histogramme\n",
    "ax2 = axes[1]\n",
    "ax2.hist(df_classes['count'], bins=30, edgecolor='black')\n",
    "ax2.set_title('Histogramme des tailles de classes', fontsize=14)\n",
    "ax2.set_xlabel('Nombre d\\'images')\n",
    "ax2.set_ylabel('FrÃ©quence')\n",
    "ax2.axvline(df_classes['count'].mean(), color='red', linestyle='--', label='Moyenne')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_DIR / 'class_distribution.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Parser les noms de classes\n",
    "def parse_class_name(name):\n",
    "    \"\"\"Extrait la plante et la condition du nom de classe\"\"\"\n",
    "    parts = name.split('___')\n",
    "    if len(parts) == 2:\n",
    "        return {\n",
    "            'plant': parts[0].replace('_', ' '),\n",
    "            'condition': parts[1].replace('_', ' '),\n",
    "            'is_healthy': 'healthy' in parts[1].lower()\n",
    "        }\n",
    "    return {\n",
    "        'plant': name,\n",
    "        'condition': 'unknown',\n",
    "        'is_healthy': None\n",
    "    }\n",
    "\n",
    "# Analyser toutes les classes\n",
    "class_info = {}\n",
    "plants = set()\n",
    "conditions = set()\n",
    "\n",
    "for class_name in classes:\n",
    "    info = parse_class_name(class_name)\n",
    "    class_info[class_name] = info\n",
    "    plants.add(info['plant'])\n",
    "    conditions.add(info['condition'])\n",
    "\n",
    "print(f\"\\nðŸŒ± Plantes uniques: {len(plants)}\")\n",
    "for p in sorted(plants):\n",
    "    print(f\"  - {p}\")\n",
    "\n",
    "print(f\"\\nðŸ¥ Conditions uniques: {len(conditions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb94322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser des exemples\n",
    "def show_samples(data_path, classes, samples_per_class=3):\n",
    "    \"\"\"Affiche des exemples de chaque classe\"\"\"\n",
    "    n_classes = min(12, len(classes))\n",
    "    fig, axes = plt.subplots(n_classes, samples_per_class, figsize=(samples_per_class * 3, n_classes * 2.5))\n",
    "    \n",
    "    for i, class_name in enumerate(classes[:n_classes]):\n",
    "        class_dir = data_path / class_name\n",
    "        images = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.JPG'))\n",
    "        \n",
    "        for j in range(samples_per_class):\n",
    "            ax = axes[i, j] if n_classes > 1 else axes[j]\n",
    "            \n",
    "            if j < len(images):\n",
    "                img = cv2.imread(str(images[j]))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                ax.imshow(img)\n",
    "            \n",
    "            ax.axis('off')\n",
    "            if j == 0:\n",
    "                ax.set_title(class_name[:30], fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PROJECT_DIR / 'sample_images.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "show_samples(data_path, classes[:12])\n",
    "\n",
    "# VÃ©rifier la qualitÃ© des images\n",
    "print(\"ðŸ” VÃ©rification de la qualitÃ© des images...\")\n",
    "\n",
    "issues = []\n",
    "image_sizes = []\n",
    "image_formats = Counter()\n",
    "\n",
    "for class_name in tqdm(classes[:10], desc=\"VÃ©rification\"):  # Limiter pour rapiditÃ©\n",
    "    class_dir = data_path / class_name\n",
    "    \n",
    "    for img_path in list(class_dir.glob('*'))[:50]:  # 50 images par classe\n",
    "        try:\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                issues.append(f\"Cannot read: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            h, w = img.shape[:2]\n",
    "            image_sizes.append((w, h))\n",
    "            image_formats[img_path.suffix.lower()] += 1\n",
    "            \n",
    "            # VÃ©rifier les dimensions minimales\n",
    "            if w < 50 or h < 50:\n",
    "                issues.append(f\"Too small ({w}x{h}): {img_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            issues.append(f\"Error {e}: {img_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Formats trouvÃ©s: {dict(image_formats)}\")\n",
    "print(f\"âš ï¸ ProblÃ¨mes dÃ©tectÃ©s: {len(issues)}\")\n",
    "\n",
    "if issues:\n",
    "    print(\"\\nExemples de problÃ¨mes:\")\n",
    "    for issue in issues[:5]:\n",
    "        print(f\"  - {issue}\")\n",
    "\n",
    "# Statistiques de taille\n",
    "sizes = np.array(image_sizes)\n",
    "print(f\"\\nðŸ“ Tailles d'images:\")\n",
    "print(f\"  Min: {sizes.min(axis=0)}\")\n",
    "print(f\"  Max: {sizes.max(axis=0)}\")\n",
    "print(f\"  Moyenne: {sizes.mean(axis=0).astype(int)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrÃ©parer le dataset final\n",
    "print(\"ðŸ“¦ PrÃ©paration du dataset final...\")\n",
    "\n",
    "# Structure cible\n",
    "TRAIN_DIR = PROCESSED_DIR / 'train'\n",
    "VAL_DIR = PROCESSED_DIR / 'val'\n",
    "TEST_DIR = PROCESSED_DIR / 'test'\n",
    "\n",
    "for d in [TRAIN_DIR, VAL_DIR, TEST_DIR]:\n",
    "    if d.exists():\n",
    "        shutil.rmtree(d)\n",
    "    d.mkdir(parents=True)\n",
    "\n",
    "# Ratios\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Copier les images\n",
    "np.random.seed(42)\n",
    "\n",
    "for class_name in tqdm(classes, desc=\"Copie des images\"):\n",
    "    src_dir = data_path / class_name\n",
    "    images = list(src_dir.glob('*.jpg')) + list(src_dir.glob('*.JPG')) + list(src_dir.glob('*.png'))\n",
    "    \n",
    "    np.random.shuffle(images)\n",
    "    \n",
    "    n = len(images)\n",
    "    n_train = int(n * TRAIN_RATIO)\n",
    "    n_val = int(n * VAL_RATIO)\n",
    "    \n",
    "    splits = {\n",
    "        'train': images[:n_train],\n",
    "        'val': images[n_train:n_train + n_val],\n",
    "        'test': images[n_train + n_val:]\n",
    "    }\n",
    "    \n",
    "    for split_name, split_images in splits.items():\n",
    "        dest_dir = PROCESSED_DIR / split_name / class_name\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for img_path in split_images:\n",
    "            shutil.copy2(img_path, dest_dir / img_path.name)\n",
    "\n",
    "# VÃ©rifier le split\n",
    "def count_split(split_dir):\n",
    "    \"\"\"Compte les images dans un split\"\"\"\n",
    "    total = 0\n",
    "    for class_dir in split_dir.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            total += len(list(class_dir.glob('*')))\n",
    "    return total\n",
    "\n",
    "print(\"\\nðŸ“Š RÃ©sumÃ© du split:\")\n",
    "print(f\"  Train: {count_split(TRAIN_DIR)} images\")\n",
    "print(f\"  Val: {count_split(VAL_DIR)} images\")\n",
    "print(f\"  Test: {count_split(TEST_DIR)} images\")\n",
    "\n",
    "# Sauvegarder les mÃ©tadonnÃ©es\n",
    "metadata = {\n",
    "    \"created_at\": pd.Timestamp.now().isoformat(),\n",
    "    \"source\": \"PlantVillage + New Plant Diseases\",\n",
    "    \"num_classes\": len(classes),\n",
    "    \"classes\": classes,\n",
    "    \"class_info\": class_info,\n",
    "    \"class_to_idx\": {name: idx for idx, name in enumerate(classes)},\n",
    "    \"splits\": {\n",
    "        \"train\": count_split(TRAIN_DIR),\n",
    "        \"val\": count_split(VAL_DIR),\n",
    "        \"test\": count_split(TEST_DIR)\n",
    "    },\n",
    "    \"plants\": list(plants),\n",
    "    \"input_size\": [224, 224],\n",
    "    \"data_path\": str(data_path)\n",
    "}\n",
    "\n",
    "# Sauvegarder dans le dossier models\n",
    "with open(MODELS_DIR / 'class_mapping.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… MÃ©tadonnÃ©es sauvegardÃ©es: {MODELS_DIR / 'class_mapping.json'}\")\n",
    "\n",
    "# Copier vers Google Drive si on est sur Colab\n",
    "if IN_COLAB:\n",
    "    import shutil\n",
    "    drive_models = Path(DRIVE_BASE) / 'drone-agri-ai' / 'models'\n",
    "    drive_models.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy(MODELS_DIR / 'class_mapping.json', drive_models)\n",
    "    \n",
    "    # Copier aussi les graphiques\n",
    "    for img in ['class_distribution.png', 'sample_images.png']:\n",
    "        src = PROJECT_DIR / img\n",
    "        if src.exists():\n",
    "            shutil.copy(src, drive_models.parent)\n",
    "    \n",
    "    print(f\"âœ… Fichiers copiÃ©s vers Google Drive: {drive_models.parent}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… PRÃ‰PARATION DES DONNÃ‰ES TERMINÃ‰E\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Classes: {len(classes)}\")\n",
    "print(f\"   Train: {count_split(TRAIN_DIR)} images\")\n",
    "print(f\"   Val: {count_split(VAL_DIR)} images\")\n",
    "print(f\"   Test: {count_split(TEST_DIR)} images\")\n",
    "print(f\"\\nâž¡ï¸ Prochaine Ã©tape: ExÃ©cuter 02_model_training.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ae1df",
   "metadata": {},
   "source": [
    "## âœ… DonnÃ©es prÃªtes!\n",
    "# \n",
    "### Le dataset est maintenant prÃªt pour l'entraÃ®nement:\n",
    "### - Images organisÃ©es en train/val/test\n",
    "### - MÃ©tadonnÃ©es sauvegardÃ©es\n",
    "### - Copie sur Google Drive"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
