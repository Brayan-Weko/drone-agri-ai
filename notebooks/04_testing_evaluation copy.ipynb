{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ee4f25",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK 04 - TESTS ET Ã‰VALUATION\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "## ðŸ§ª Tests et Ã‰valuation du ModÃ¨le\n",
    "# \n",
    "# Ce notebook couvre:\n",
    "# 1. Ã‰valuation sur le jeu de test\n",
    "# 2. Matrice de confusion\n",
    "# 3. MÃ©triques par classe\n",
    "# 4. Analyse des erreurs\n",
    "# 5. Tests sur images rÃ©elles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "!pip install -q tensorflow scikit-learn seaborn\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modÃ¨le et les donnÃ©es\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "MODEL_DIR = Path('/content/drive/MyDrive/drone-agri-ai/models')\n",
    "DATA_DIR = Path('/content/drive/MyDrive/drone-agri-ai/data')\n",
    "TEST_DIR = DATA_DIR / 'test'\n",
    "\n",
    "# Charger le modÃ¨le\n",
    "model = keras.models.load_model(MODEL_DIR / 'plant_model.keras')\n",
    "\n",
    "# Charger le mapping des classes\n",
    "with open(MODEL_DIR / 'class_mapping.json', 'r') as f:\n",
    "    class_mapping = json.load(f)\n",
    "\n",
    "class_names = class_mapping['class_names']\n",
    "num_classes = len(class_names)\n",
    "print(f\"Classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabee0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrÃ©parer le gÃ©nÃ©rateur de test\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "test_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    preprocessing_function=lambda x: (x - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Images de test: {test_generator.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrÃ©dictions sur le jeu de test\n",
    "print(\"ðŸ”® GÃ©nÃ©ration des prÃ©dictions...\")\n",
    "\n",
    "# Pour le modÃ¨le multi-sorties, on se concentre sur la classification\n",
    "predictions = model.predict(test_generator, verbose=1)\n",
    "\n",
    "# Extraire les prÃ©dictions de classification\n",
    "if isinstance(predictions, dict):\n",
    "    class_predictions = predictions['classification']\n",
    "else:\n",
    "    class_predictions = predictions[1]  # Index de la sortie classification\n",
    "\n",
    "y_pred = np.argmax(class_predictions, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "print(f\"PrÃ©dictions: {y_pred.shape}\")\n",
    "print(f\"Labels: {y_true.shape}\")\n",
    "\n",
    "# Accuracy globale\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nðŸ“Š ACCURACY GLOBALE: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nðŸ“‹ RAPPORT DE CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "report = classification_report(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Afficher les mÃ©triques principales\n",
    "print(f\"\\nAccuracy: {report['accuracy']:.4f}\")\n",
    "print(f\"Macro Avg Precision: {report['macro avg']['precision']:.4f}\")\n",
    "print(f\"Macro Avg Recall: {report['macro avg']['recall']:.4f}\")\n",
    "print(f\"Macro Avg F1-Score: {report['macro avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3066fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 meilleures et pires classes\n",
    "class_metrics = []\n",
    "for class_name in class_names:\n",
    "    if class_name in report:\n",
    "        class_metrics.append({\n",
    "            'class': class_name,\n",
    "            'precision': report[class_name]['precision'],\n",
    "            'recall': report[class_name]['recall'],\n",
    "            'f1': report[class_name]['f1-score'],\n",
    "            'support': report[class_name]['support']\n",
    "        })\n",
    "\n",
    "df_metrics = pd.DataFrame(class_metrics)\n",
    "df_metrics = df_metrics.sort_values('f1', ascending=False)\n",
    "\n",
    "print(\"\\nðŸ† TOP 10 MEILLEURES CLASSES:\")\n",
    "print(df_metrics.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nâš ï¸ TOP 10 PIRES CLASSES:\")\n",
    "print(df_metrics.tail(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d1406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion (version rÃ©duite)\n",
    "# Pour 38 classes, la matrice complÃ¨te est trop grande\n",
    "# On va crÃ©er une version agrÃ©gÃ©e par plante\n",
    "\n",
    "def get_plant_name(class_name):\n",
    "    \"\"\"Extrait le nom de la plante\"\"\"\n",
    "    parts = class_name.split('___')\n",
    "    return parts[0] if len(parts) >= 1 else class_name\n",
    "\n",
    "# Mapper les classes aux plantes\n",
    "plant_mapping = {i: get_plant_name(name) for i, name in enumerate(class_names)}\n",
    "plants = sorted(set(plant_mapping.values()))\n",
    "plant_to_idx = {p: i for i, p in enumerate(plants)}\n",
    "\n",
    "# Convertir les labels en index de plantes\n",
    "y_true_plants = [plant_to_idx[plant_mapping[y]] for y in y_true]\n",
    "y_pred_plants = [plant_to_idx[plant_mapping[y]] for y in y_pred]\n",
    "\n",
    "# Matrice de confusion par plante\n",
    "cm_plants = confusion_matrix(y_true_plants, y_pred_plants)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm_plants, \n",
    "    annot=True, \n",
    "    fmt='d',\n",
    "    xticklabels=plants,\n",
    "    yticklabels=plants,\n",
    "    cmap='Blues'\n",
    ")\n",
    "plt.xlabel('PrÃ©dit')\n",
    "plt.ylabel('RÃ©el')\n",
    "plt.title('Matrice de confusion par type de plante')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'confusion_matrix_plants.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Analyser les erreurs frÃ©quentes\n",
    "print(\"\\nðŸ” ANALYSE DES ERREURS FRÃ‰QUENTES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "errors = []\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i] != y_pred[i]:\n",
    "        errors.append({\n",
    "            'true': class_names[y_true[i]],\n",
    "            'pred': class_names[y_pred[i]],\n",
    "            'true_plant': get_plant_name(class_names[y_true[i]]),\n",
    "            'pred_plant': get_plant_name(class_names[y_pred[i]])\n",
    "        })\n",
    "\n",
    "df_errors = pd.DataFrame(errors)\n",
    "\n",
    "# Erreurs les plus frÃ©quentes\n",
    "error_counts = df_errors.groupby(['true', 'pred']).size().reset_index(name='count')\n",
    "error_counts = error_counts.sort_values('count', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 confusions les plus frÃ©quentes:\")\n",
    "for _, row in error_counts.head(15).iterrows():\n",
    "    print(f\"  {row['true'][:30]:30} -> {row['pred'][:30]:30} ({row['count']}x)\")\n",
    "\n",
    "# Visualiser les erreurs\n",
    "print(\"\\nðŸ“¸ VISUALISATION DES ERREURS\")\n",
    "\n",
    "# Trouver des exemples d'erreurs\n",
    "error_indices = np.where(y_true != y_pred)[0]\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(error_indices[:12]):\n",
    "    # Charger l'image\n",
    "    img_path = test_generator.filepaths[idx]\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    ax = axes[i]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"RÃ©el: {class_names[y_true[idx]][:20]}\\nPrÃ©dit: {class_names[y_pred[idx]][:20]}\", fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'error_examples.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3eff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester le modÃ¨le TFLite\n",
    "print(\"\\nðŸ”§ TEST DU MODÃˆLE TFLITE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tflite_path = MODEL_DIR / 'plant_model.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(f\"Input: {input_details[0]['shape']}, {input_details[0]['dtype']}\")\n",
    "print(f\"Outputs: {len(output_details)}\")\n",
    "\n",
    "# Tester sur quelques images\n",
    "def predict_tflite(image_path):\n",
    "    \"\"\"PrÃ©dit avec le modÃ¨le TFLite\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Normalisation\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = (img - mean) / std\n",
    "    \n",
    "    img = np.expand_dims(img, axis=0).astype(np.float32)\n",
    "    \n",
    "    interpreter.set_tensor(input_details[0]['index'], img)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # RÃ©cupÃ©rer les sorties\n",
    "    outputs = {}\n",
    "    for detail in output_details:\n",
    "        name = detail['name']\n",
    "        tensor = interpreter.get_tensor(detail['index'])\n",
    "        outputs[name] = tensor\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Tester\n",
    "sample_images = list(TEST_DIR.rglob('*.jpg'))[:5]\n",
    "for img_path in sample_images:\n",
    "    outputs = predict_tflite(str(img_path))\n",
    "    \n",
    "    # Trouver la sortie de classification\n",
    "    for name, tensor in outputs.items():\n",
    "        if 'classification' in name.lower() or tensor.shape[-1] == num_classes:\n",
    "            pred_idx = np.argmax(tensor[0])\n",
    "            confidence = np.max(tensor[0])\n",
    "            print(f\"{img_path.parent.name} -> {class_names[pred_idx]} ({confidence*100:.1f}%)\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport final\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š RAPPORT FINAL D'Ã‰VALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = {\n",
    "    \"accuracy\": float(accuracy),\n",
    "    \"num_classes\": num_classes,\n",
    "    \"test_samples\": len(y_true),\n",
    "    \"macro_precision\": float(report['macro avg']['precision']),\n",
    "    \"macro_recall\": float(report['macro avg']['recall']),\n",
    "    \"macro_f1\": float(report['macro avg']['f1-score']),\n",
    "    \"num_errors\": len(errors),\n",
    "    \"error_rate\": len(errors) / len(y_true),\n",
    "    \"best_classes\": df_metrics.head(5)['class'].tolist(),\n",
    "    \"worst_classes\": df_metrics.tail(5)['class'].tolist()\n",
    "}\n",
    "\n",
    "print(f\"\"\"\n",
    "Accuracy globale:     {summary['accuracy']*100:.2f}%\n",
    "Classes:              {summary['num_classes']}\n",
    "Ã‰chantillons test:    {summary['test_samples']}\n",
    "\n",
    "Macro Precision:      {summary['macro_precision']:.4f}\n",
    "Macro Recall:         {summary['macro_recall']:.4f}\n",
    "Macro F1-Score:       {summary['macro_f1']:.4f}\n",
    "\n",
    "Erreurs:              {summary['num_errors']} ({summary['error_rate']*100:.2f}%)\n",
    "\n",
    "Meilleures classes:   {', '.join(summary['best_classes'][:3])}\n",
    "Pires classes:        {', '.join(summary['worst_classes'][:3])}\n",
    "\"\"\")\n",
    "\n",
    "# Sauvegarder le rapport\n",
    "with open(MODEL_DIR / 'evaluation_report.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"âœ… Rapport sauvegardÃ©!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
